{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rmuJVrzRnh8V955d8cxZtJuKmOpGCygr","timestamp":1765138842877},{"file_id":"1RZlG2ZuqjwUXDe2F7aiy7d7IwFhVUR6z","timestamp":1760237714862}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CIS 5450 Project: Difficulty Topics\n","**Group Members:**\n","* **Alon Jacoby**\n","* **Trey Elder**\n","* **Nicky Desai**\n","\n","> This notebook documents how we implemented difficulty topics in our project. Use the link button in the top right when you select a cell to get a **hyperlink**.\n"],"metadata":{"id":"G7fXCyFHPr3X"}},{"cell_type":"markdown","source":["https://colab.research.google.com/drive/1DdtT9NmdUAO-zXHJ9C5HkocWQOC8MWhv"],"metadata":{"id":"JgXqyc811U4-"}},{"cell_type":"markdown","source":["## Topic 1: Feature Engineering\n","[Hyperlink](\n","  https://colab.research.google.com/drive/1DdtT9NmdUAO-zXHJ9C5HkocWQOC8MWhv#scrollTo=cM49fbucqg11&line=1&uniqifier=1)\n","\n","### **Why we used this concept**\n","The raw Spotify dataset contains audio features, metadata, and track names—but many of these variables do not directly encode the underlying musical or structural characteristics of each song. From our density plots and exploratory analysis, we observed:\n","\n","* Several audio variables are quasi-binary or clustered, suggesting they should be binned rather than treated as continuous  \n","* Categorical audio metadata (`key`, `mode`, `time_signature`) do not carry numeric meaning without transformation  \n","* Artist-level patterns (frequency of appearance, stylistic consistency) influence genre identity but are not encoded in the base attributes  \n","\n","Feature engineering allows us to transform raw inputs into **model-ready signals** that better reflect musically meaningful structure. These transformations improve clustering behavior, model performance, and interpretability.\n","\n","### **How we implemented it**\n","We engineered four major families of features:\n","\n","#### 1. Binning Quasi-Binary Audio Features\n","* **Motivation:** Density plots showed strong bimodality for `instrumentalness`, `acousticness`, and `speechiness`.  \n","* **Process:**\n","  - Convert `instrumentalness` → binary (`> 0.5`)\n","  - Convert `acousticness` → binary (`> 0.5`)\n","  - Convert `speechiness` → 3-level ordinal category  \n","* **Output:** Clean categorical variables that reflect meaningful musical distinctions.\n","\n","#### 2. One-Hot Encoding of Categorical Features\n","* **Encoder:** `OneHotEncoder(sparse_output=False, handle_unknown='ignore')`  \n","* **Categorical features encoded:**  \n","  - `mode`, `time_signature`, `key`  \n","  - engineered `speechiness`, `instrumentalness`, `acousticness`\n","* **Output:** A fully numeric representation of categorical musical attributes.\n","\n","#### 3. Higher-Level Engineered Features (DuckDB)\n","* **Method:** SQL query executed via DuckDB  \n","* **Features created:**  \n","  - `artist_track_count`  \n","  - `energy_danceability` interaction term  \n","  - `title_length` (character count)  \n","* **Output:** Contextual features capturing artist prominence, rhythmic intensity, and titling patterns.\n","\n","#### 4. Artist Group Identification (Regex-Based Feature)\n","* **Motivation:** Artist names often include structural cues indicating solo vs. group performers (“&”, “and”, “band”, “trio”).  \n","* **Output:** `artist_is_group` boolean variable for distinguishing ensemble-based artists.\n","\n","### **Results & Interpretation**\n","- **Binned and one-hot encoded features** clarified the structure of audio attributes, improving the model’s interpretability around “instrumental vs. vocal,” “acoustic vs. electronic,” and “spoken vs. musical” distinctions.\n","- **DuckDB-derived features** enriched the dataset with musically relevant context, such as artist prominence, rhythmic interactions, and title styling cues.\n","- **Artist group identification** introduced a new dimension of interpretability, helping models differentiate ensemble-driven genres from solo-driven styles.\n","\n","These engineered features are used directly in the modeling pipeline (Part 5) and help explain several patterns observed in our results—such as clusters dominated by acoustic singer-songwriters, high-energy rock bands, or speech-heavy spoken-word tracks.\n","\n","\n","\n"],"metadata":{"id":"Y2frpO7QfMxt"}},{"cell_type":"markdown","source":["## Topic 2: Entity Linking / External Model Integration  \n","[Hyperlink](\n","  https://colab.research.google.com/drive/1DdtT9NmdUAO-zXHJ9C5HkocWQOC8MWhv#scrollTo=ZM-SoQ0PBrsP)\n","### **Why we used this concept**\n","Track titles contain rich implicit information about genre, mood, theme, and stylistic intent—yet this meaning is not available in numerical form within the raw dataset. To meaningfully incorporate this semantic signal, we needed a method to **link each Spotify record to external knowledge** that captures linguistic and contextual patterns learned from large text corpora.\n","\n","Record linking via pretrained language models allows us to enrich the dataset with **semantic embeddings** that encode:\n","- emotional tone  \n","- genre-specific keywords  \n","- cultural or thematic associations  \n","- stylistic similarities between tracks  \n","\n","Integrating this external semantic representation provides information not captured by audio features alone and strengthens downstream clustering and modeling tasks.\n","\n","### **How we implemented it**\n","We applied a transformer-based encoding pipeline using the pretrained `all-MiniLM-L6-v2` model from the Sentence-Transformers library. The process involved:\n","\n","1. **Tokenizing track titles** into model-ready sequences  \n","2. **Encoding titles using the transformer**, leveraging either GPU or CPU depending on availability  \n","3. **Applying mean pooling** to obtain a single dense vector per track  \n","4. **Batch processing** to ensure efficiency  \n","5. **Appending the resulting embeddings** (384-dimensional vectors) back into the dataset as new semantic features  \n","\n","The transformer acts as an *external knowledge source*, effectively linking each track to a position in a semantic space learned from millions of documents.\n","\n","### **Results & Interpretation**\n","- These embeddings captured song-level meaning far better than raw text, enabling the model to differentiate between thematic categories such as:\n","  - reflective singer-songwriter ballads  \n","  - high-energy party songs  \n","  - orchestral or classical works  \n","  - country tracks with genre-specific vocabulary  \n","- The enriched dataset allowed clusters to form along **semantic as well as acoustic axes**, leading to more coherent genre-like groupings.\n","- The external model integration improved interpretability by revealing connections between text-derived and audio-derived patterns.\n","\n","Overall, this record linking step added a powerful second modality to our dataset, enabling the project to use textual semantics alongside audio structure to produce deeper insights and better model performance.\n"],"metadata":{"id":"ZJO1cZCya8CB"}},{"cell_type":"markdown","source":["## Topic 3: Hyperparameter Tuning\n","[Hyperlink](https://colab.research.google.com/drive/1DdtT9NmdUAO-zXHJ9C5HkocWQOC8MWhv#scrollTo=FUdNqETlEugx\n","  )\n","\n","### **Why we used this concept**\n","Clustering algorithms such as **DBSCAN** and **K-Means** are highly sensitive to their configuration settings. Small changes in parameters can completely alter cluster structure, cause groups to collapse, or produce meaningless partitions—especially in a feature space as complex as ours, which blends audio descriptors with semantic embeddings.\n","\n","To ensure our clusters reflected **true underlying musical and semantic patterns**, rather than arbitrary defaults, we used hyperparameter tuning to systematically search for the settings that produced the strongest—and most interpretable—structure. Tuning allowed us to:\n","\n","- identify density thresholds appropriate for high-dimensional embeddings  \n","- prevent DBSCAN from assigning all points to noise  \n","- find stable values for `eps`, `min_samples`, and number of clusters  \n","- evaluate model behavior across different distance metrics  \n","- strengthen alignment between clusters and known genre patterns  \n","\n","Hyperparameter tuning was therefore essential for producing **meaningful, robust, and musically coherent** clusters.\n","\n","### **How we implemented it**\n","We carried out two forms of hyperparameter tuning tailored to the needs of each clustering method:\n","\n","1. **Sweeping over the number of clusters for K-Means**  \n","   - Evaluated a range of `k` values on both raw and standardized features  \n","   - Computed **inertia** and **homogeneity score** for each `k`  \n","   - Identified cluster counts that balanced compactness with interpretability  \n","\n","2. **Grid search for DBSCAN hyperparameters**  \n","   - Constructed a search grid for `eps` and `min_samples`  \n","   - Fit DBSCAN on a subsampled training set for each combination  \n","   - Required at least two clusters to apply the silhouette score  \n","   - Selected the configuration that maximized **silhouette score**  \n","   - Functioned as a manual, unsupervised analog to **GridSearchCV**  \n","\n","3. **Bayesian Optimization for DBSCAN hyperparameters**\n","   - Based on prior results from grid search, allowed for a more fine-grained search in a subset of parameters.\n","   - Fit DBSCAN on 50 iterations of the search.\n","   - Optimization problem defined as maximization of silouhette score.\n","\n","These tuning strategies allowed us to explore parameter landscapes systematically and avoid unreliable, default-driven outcomes.\n","\n","### **Results & Interpretation**\n","- **DBSCAN**  \n","  - Default settings produced trivial or unstable clustering (e.g., excessive noise points).  \n","  - Tuning revealed parameter regions where DBSCAN discovered **clear density-based groupings** consistent with musical structure—such as acoustic ballads, electronic tracks, orchestral pieces, and lyric-centric pop.  \n","  - The optimal configuration balanced local density sensitivity with global separability.\n","\n","- **K-Means**  \n","  - Varying `k` showed that certain cluster counts aligned far better with genre labels and the semantic–acoustic structure of the dataset.  \n","  - Improvements in inertia and homogeneity indicated that tuning `k` helped K-Means partition the space more coherently.  \n","  - Overly small or overly large `k` values degraded interpretability—tuning identified a reasonable middle ground.\n","\n","Overall, hyperparameter tuning transformed clustering from a **parameter-guessing exercise** into a structured, data-driven process, enabling our models to produce richer, more interpretable, and musically meaningful insights.\n","\n"],"metadata":{"id":"n3_zIjway_39"}},{"cell_type":"code","source":[],"metadata":{"id":"FN3L68SrJ3FB"},"execution_count":null,"outputs":[]}]}